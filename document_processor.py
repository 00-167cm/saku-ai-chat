"""
ğŸ“„ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†
    PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ã€ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã™ã‚‹
    
ã€å½¹å‰²ã€‘
- PDFãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
- ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
- ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ï¼ˆRAGç”¨ã«é©åˆ‡ãªã‚µã‚¤ã‚ºã«ï¼‰
- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä»˜ä¸ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã€ãƒšãƒ¼ã‚¸ç•ªå·ãªã©ï¼‰
"""
import os
from typing import List, Dict, Any
from pathlib import Path

# PDFå‡¦ç†ç”¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
import fitz  # PyMuPDF

# HTMLå‡¦ç†ç”¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
from bs4 import BeautifulSoup

# LangChainã®ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²
from langchain_text_splitters import RecursiveCharacterTextSplitter


class DocumentProcessor:
    """
    ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã‚¯ãƒ©ã‚¹
    PDFã‚’ãƒ†ã‚­ã‚¹ãƒˆåŒ–ã—ã¦ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã™ã‚‹
    
    ã€ã“ã®ã‚¯ãƒ©ã‚¹ãŒæŒã¤ãƒ‡ãƒ¼ã‚¿ã€‘
    - self.chunk_size: 1ãƒãƒ£ãƒ³ã‚¯ã®æœ€å¤§æ–‡å­—æ•°
    - self.chunk_overlap: ãƒãƒ£ãƒ³ã‚¯é–“ã®é‡è¤‡æ–‡å­—æ•°
    - self.text_splitter: ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²å™¨ï¼ˆLangChainï¼‰
    """
    
    def __init__(
        self,
        chunk_size: int = 500,
        chunk_overlap: int = 100
    ):
        """
        ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã®åˆæœŸåŒ–
        
        ã€å‡¦ç†å†…å®¹ã€‘
        1. ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã‚’è¨­å®š
        2. ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²å™¨ã‚’åˆæœŸåŒ–
        
        Args:
            chunk_size: 1ãƒãƒ£ãƒ³ã‚¯ã®æœ€å¤§æ–‡å­—æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ500æ–‡å­—ï¼‰
            chunk_overlap: ãƒãƒ£ãƒ³ã‚¯é–“ã®é‡è¤‡æ–‡å­—æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ100æ–‡å­—ï¼‰
        
        ã€ãªãœé‡è¤‡ã•ã›ã‚‹ã®ï¼Ÿã€‘
        æ–‡ã®é€”ä¸­ã§åˆ‡ã‚Œã‚‹ã¨æ„å‘³ãŒå¤±ã‚ã‚Œã‚‹ãŸã‚ã€
        å‰å¾Œã®ãƒãƒ£ãƒ³ã‚¯ã¨å°‘ã—é‡è¤‡ã•ã›ã‚‹ã“ã¨ã§
        æ–‡è„ˆã‚’ä¿æŒã™ã‚‹
        
        ã€å‘¼ã³å‡ºã—ä¾‹ã€‘
        processor = DocumentProcessor(chunk_size=500, chunk_overlap=100)
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        
        # ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²å™¨ã‚’åˆæœŸåŒ–
        # RecursiveCharacterTextSplitter: æ®µè½â†’æ–‡â†’å˜èªã®é †ã§åˆ†å‰²ã‚’è©¦ã¿ã‚‹
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            # æ—¥æœ¬èªå¯¾å¿œã®ã‚»ãƒ‘ãƒ¬ãƒ¼ã‚¿ãƒ¼
            separators=["\n\n", "\n", "ã€‚", "ã€", " ", ""]
        )
    
    def extract_text_from_pdf(self, pdf_path: str) -> List[Dict[str, Any]]:
        """
        PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºï¼ˆãƒšãƒ¼ã‚¸ã”ã¨ï¼‰
        
        ã€å‡¦ç†ã®æµã‚Œã€‘
        1. PyMuPDFã§PDFã‚’é–‹ã
        2. å„ãƒšãƒ¼ã‚¸ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
        3. ãƒšãƒ¼ã‚¸ç•ªå·ã¨ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä»˜ä¸
        
        Args:
            pdf_path: PDFãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        
        Returns:
            ãƒšãƒ¼ã‚¸ã”ã¨ã®ãƒ†ã‚­ã‚¹ãƒˆã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
            [
                {
                    "text": "ãƒšãƒ¼ã‚¸1ã®ãƒ†ã‚­ã‚¹ãƒˆ...",
                    "metadata": {
                        "source": "document.pdf",
                        "page": 1
                    }
                },
                ...
            ]
        
        ã€å‘¼ã³å‡ºã—ä¾‹ã€‘
        pages = processor.extract_text_from_pdf("data/documents/rules.pdf")
        """
        # çµæœã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆ
        pages = []
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å–å¾—ï¼ˆãƒ‘ã‚¹ã‹ã‚‰ï¼‰
        file_name = Path(pdf_path).name
        
        try:
            # PDFã‚’é–‹ã
            doc = fitz.open(pdf_path)
            
            # å„ãƒšãƒ¼ã‚¸ã‚’å‡¦ç†
            for page_num, page in enumerate(doc, start=1):
                # ãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
                text = page.get_text()
                
                # ç©ºã§ãªã‘ã‚Œã°è¿½åŠ 
                if text.strip():
                    pages.append({
                        "text": text,
                        "metadata": {
                            "source": file_name,
                            "page": page_num
                        }
                    })
            
            # PDFã‚’é–‰ã˜ã‚‹
            doc.close()
            
            print(f"âœ… PDFèª­ã¿è¾¼ã¿å®Œäº†: {file_name} ({len(pages)}ãƒšãƒ¼ã‚¸)")
            return pages
            
        except Exception as e:
            print(f"âŒ PDFèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {pdf_path}")
            print(f"   ã‚¨ãƒ©ãƒ¼å†…å®¹: {e}")
            return []

    def extract_text_from_html(self, html_path: str) -> List[Dict[str, Any]]:
        """
        HTMLã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º

        ã€å‡¦ç†ã®æµã‚Œã€‘
        1. HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
        2. BeautifulSoupã§HTMLã‚’è§£æ
        3. ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚’æŠ½å‡ºï¼ˆã‚¿ã‚°ã‚’é™¤å»ï¼‰
        4. ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä»˜ä¸

        Args:
            html_path: HTMLãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹

        Returns:
            ãƒ†ã‚­ã‚¹ãƒˆã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
            [
                {
                    "text": "HTMLã®ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹...",
                    "metadata": {
                        "source": "document.html",
                        "page": 1
                    }
                }
            ]

        ã€å‘¼ã³å‡ºã—ä¾‹ã€‘
        pages = processor.extract_text_from_html("data/documents/rules.html")
        """
        # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å–å¾—ï¼ˆãƒ‘ã‚¹ã‹ã‚‰ï¼‰
        file_name = Path(html_path).name

        try:
            # HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
            with open(html_path, 'r', encoding='utf-8') as f:
                html_content = f.read()

            # BeautifulSoupã§HTMLã‚’è§£æ
            soup = BeautifulSoup(html_content, 'html.parser')

            # scriptã‚¿ã‚°ã¨styleã‚¿ã‚°ã‚’é™¤å»
            for script in soup(['script', 'style']):
                script.decompose()

            # ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚’æŠ½å‡º
            text = soup.get_text()

            # ä½™åˆ†ãªç©ºç™½ã‚’å‰Šé™¤
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            text = '\n'.join(chunk for chunk in chunks if chunk)

            # ç©ºã§ãªã‘ã‚Œã°è¿”ã™
            if text.strip():
                print(f"âœ… HTMLèª­ã¿è¾¼ã¿å®Œäº†: {file_name}")
                return [{
                    "text": text,
                    "metadata": {
                        "source": file_name,
                        "page": 1
                    }
                }]
            else:
                print(f"âš ï¸ ãƒ†ã‚­ã‚¹ãƒˆãŒæŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ: {file_name}")
                return []

        except Exception as e:
            print(f"âŒ HTMLèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {html_path}")
            print(f"   ã‚¨ãƒ©ãƒ¼å†…å®¹: {e}")
            return []

    def split_into_chunks(
        self,
        pages: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        ãƒšãƒ¼ã‚¸ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
        
        ã€å‡¦ç†ã®æµã‚Œã€‘
        1. å„ãƒšãƒ¼ã‚¸ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
        2. text_splitterã§ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
        3. å„ãƒãƒ£ãƒ³ã‚¯ã«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä»˜ä¸
        
        Args:
            pages: extract_text_from_pdf()ã®æˆ»ã‚Šå€¤
        
        Returns:
            ãƒãƒ£ãƒ³ã‚¯ã”ã¨ã®ãƒ†ã‚­ã‚¹ãƒˆã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
            [
                {
                    "text": "ãƒãƒ£ãƒ³ã‚¯1ã®ãƒ†ã‚­ã‚¹ãƒˆ...",
                    "metadata": {
                        "source": "document.pdf",
                        "page": 1,
                        "chunk_index": 0
                    }
                },
                ...
            ]
        
        ã€å‘¼ã³å‡ºã—ä¾‹ã€‘
        chunks = processor.split_into_chunks(pages)
        """
        chunks = []
        chunk_index = 0
        
        for page in pages:
            # ãƒšãƒ¼ã‚¸ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
            page_chunks = self.text_splitter.split_text(page["text"])
            
            # å„ãƒãƒ£ãƒ³ã‚¯ã«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä»˜ä¸
            for chunk_text in page_chunks:
                chunks.append({
                    "text": chunk_text,
                    "metadata": {
                        **page["metadata"],  # å…ƒã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ã‚³ãƒ”ãƒ¼
                        "chunk_index": chunk_index
                    }
                })
                chunk_index += 1
        
        print(f"âœ… ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²å®Œäº†: {len(chunks)}ãƒãƒ£ãƒ³ã‚¯")
        return chunks
    
    def process_pdf(self, pdf_path: str) -> List[Dict[str, Any]]:
        """
        PDFã‚’å‡¦ç†ã—ã¦ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ï¼ˆä¸€é€£ã®å‡¦ç†ã‚’ã¾ã¨ã‚ã¦å®Ÿè¡Œï¼‰
        
        ã€å‡¦ç†ã®æµã‚Œã€‘
        1. PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
        2. ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
        3. çµæœã‚’è¿”ã™
        
        Args:
            pdf_path: PDFãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        
        Returns:
            ãƒãƒ£ãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆï¼ˆsplit_into_chunks()ã¨åŒã˜å½¢å¼ï¼‰
        
        ã€å‘¼ã³å‡ºã—ä¾‹ã€‘
        chunks = processor.process_pdf("data/documents/rules.pdf")
        """
        # ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
        pages = self.extract_text_from_pdf(pdf_path)
        
        if not pages:
            return []
        
        # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
        chunks = self.split_into_chunks(pages)

        return chunks

    def process_html(self, html_path: str) -> List[Dict[str, Any]]:
        """
        HTMLã‚’å‡¦ç†ã—ã¦ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ï¼ˆä¸€é€£ã®å‡¦ç†ã‚’ã¾ã¨ã‚ã¦å®Ÿè¡Œï¼‰

        ã€å‡¦ç†ã®æµã‚Œã€‘
        1. HTMLã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
        2. ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
        3. çµæœã‚’è¿”ã™

        Args:
            html_path: HTMLãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹

        Returns:
            ãƒãƒ£ãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆï¼ˆsplit_into_chunks()ã¨åŒã˜å½¢å¼ï¼‰

        ã€å‘¼ã³å‡ºã—ä¾‹ã€‘
        chunks = processor.process_html("data/documents/rules.html")
        """
        # ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
        pages = self.extract_text_from_html(html_path)

        if not pages:
            return []

        # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
        chunks = self.split_into_chunks(pages)

        return chunks

    def process_directory(self, directory_path: str) -> List[Dict[str, Any]]:
        """
        ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®å…¨PDFã¨HTMLã‚’å‡¦ç†

        ã€å‡¦ç†ã®æµã‚Œã€‘
        1. ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®.pdfã¨.htmlãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
        2. å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
        3. å…¨ãƒãƒ£ãƒ³ã‚¯ã‚’ã¾ã¨ã‚ã¦è¿”ã™

        Args:
            directory_path: ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ ¼ç´ã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹

        Returns:
            å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ã¾ã¨ã‚ãŸãƒªã‚¹ãƒˆ

        ã€å‘¼ã³å‡ºã—ä¾‹ã€‘
        all_chunks = processor.process_directory("data/documents")
        """
        all_chunks = []

        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
        directory = Path(directory_path)

        if not directory.exists():
            print(f"âŒ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {directory_path}")
            return []

        # .pdfã¨.htmlãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
        pdf_files = list(directory.glob("*.pdf"))
        html_files = list(directory.glob("*.html"))

        total_files = len(pdf_files) + len(html_files)

        if total_files == 0:
            print(f"âš ï¸ PDFã¾ãŸã¯HTMLãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {directory_path}")
            return []

        print(f"ğŸ“ {len(pdf_files)}å€‹ã®PDFãƒ•ã‚¡ã‚¤ãƒ«ã€{len(html_files)}å€‹ã®HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡º")

        # å„PDFã‚’å‡¦ç†
        for pdf_file in pdf_files:
            chunks = self.process_pdf(str(pdf_file))
            all_chunks.extend(chunks)

        # å„HTMLã‚’å‡¦ç†
        for html_file in html_files:
            chunks = self.process_html(str(html_file))
            all_chunks.extend(chunks)

        print(f"âœ… å…¨ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å®Œäº†: åˆè¨ˆ{len(all_chunks)}ãƒãƒ£ãƒ³ã‚¯")
        return all_chunks
